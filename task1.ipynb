import pandas as pd
import dask.dataframe as dd
from sklearn.datasets import load_iris

# Step 1: Load inbuilt Iris dataset from scikit-learn
iris = load_iris()
iris_df = pd.DataFrame(iris.data, columns=iris.feature_names)
iris_df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)

# Step 2: Simulate large dataset by repeating the Iris data
large_df = pd.concat([iris_df] * 10000, ignore_index=True)  # ~1.5 million rows

# Step 3: Convert to Dask DataFrame
ddf = dd.from_pandas(large_df, npartitions=10)

# Step 4: Perform analysis with Dask
# Count rows per species
species_counts = ddf.groupby('species').size().compute()
print("Species Count:\n", species_counts)

# Average measurements per species
avg_features = ddf.groupby('species').mean().compute()
print("\nAverage Feature Values by Species:\n", avg_features)

# Step 5: Describe statistics
summary_stats = ddf.describe().compute()
print("\nSummary Statistics:\n", summary_stats)

# Step 6: Optional - Save to CSV
species_counts.to_csv("species_counts.csv")
avg_features.to_csv("avg_features.csv")
summary_stats.to_csv("summary_stats.csv")
